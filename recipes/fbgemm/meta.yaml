{% set name = "fbgemm" %}
{% set version = "1.2.0" %}

package:
  name: {{ name|lower }}
  version: {{ version }}

source:
  git_url: https://github.com/pytorch/FBGEMM.git
  git_rev: v{{ version }}

build:
  number: 0
  skip: true  # [py<38]
  skip: true  # [win]
  skip: true  # [aarch64]  # git_url source requires git on build system, problematic for cross-compilation

outputs:
  - name: fbgemm-gpu
    build:
      script: |
        cd fbgemm_gpu
        {% if cuda_compiler_version == "None" %}
        python setup.py --package_variant=cpu --package_channel=release install --prefix=$PREFIX --single-version-externally-managed --record=record.txt
        {% else %}
        python setup.py --package_variant=cuda --package_channel=release install --prefix=$PREFIX --single-version-externally-managed --record=record.txt
        {% endif %}
      script_env:
        # Set CUDA architectures: starts at 6.0 because code uses __hfma2 (half-precision FMA)
        # which requires Compute Capability 6.0+ (Pascal). CC 5.0 (Maxwell) only supports FP16 storage, not arithmetic.
        # Compiling with 10.0 failed with error: nvcc fatal: Unsupported gpu architecture 'compute_100' - cuda 12.6 issue ?
        - TORCH_CUDA_ARCH_LIST=6.0;7.0;7.5;8.0;8.6;9.0+PTX  # [cuda_compiler_version != "None"]
      string: cuda{{ cuda_compiler_version | replace('.', '') }}_py{{ CONDA_PY }}_h{{ PKG_HASH }}_{{ PKG_BUILDNUM }}  # [cuda_compiler_version != "None"]
      string: cpu_py{{ CONDA_PY }}_h{{ PKG_HASH }}_{{ PKG_BUILDNUM }}  # [cuda_compiler_version == "None"]
    requirements:
      build:
        - {{ compiler('c') }}
        - {{ compiler('cxx') }}
        - {{ compiler('cuda') }}  # [cuda_compiler_version != "None"]
        - {{ stdlib('c') }}
        - cmake
        - make
        - ninja
        - git
        - python
        - pip
        - setuptools-git-versioning
        - pytorch
        - pytorch * *cuda*  # [cuda_compiler_version != "None"]
        - scikit-build
        - tabulate
        - jinja2
        - pyyaml
        - cuda-cudart-dev  # [cuda_compiler_version != "None"]
        - cuda-nvrtc-dev  # [cuda_compiler_version != "None"]
        - cuda-nvtx-dev  # [cuda_compiler_version != "None"]
        - libcublas-dev  # [cuda_compiler_version != "None"]
        - libcusolver-dev  # [cuda_compiler_version != "None"]
        - libcusparse-dev  # [cuda_compiler_version != "None"]
        - libcurand-dev  # [cuda_compiler_version != "None"]
      host:
        - python
        - pip
        - setuptools
        - setuptools-git-versioning
        - wheel
        - pytorch
        - scikit-build
        - numpy
        - cuda-version {{ cuda_compiler_version }}  # [cuda_compiler_version != "None"]
      run:
        - python
        - pytorch
        - numpy
        - cuda-version >={{ cuda_compiler_version }}  # [cuda_compiler_version != "None"]
        - cuda-cudart  # [cuda_compiler_version != "None"]
    test:
      imports:
        - fbgemm_gpu
      commands:
        - python -c "import fbgemm_gpu; print(fbgemm_gpu.__version__)"

  - name: fbgemm-gpu-genai
    build:
      skip: true  # [cuda_compiler_version == "None"]
      script: |
        cd fbgemm_gpu
        python setup.py --package_variant=genai --package_channel=release install --prefix=$PREFIX --single-version-externally-managed --record=record.txt
      script_env:
        # Set CUDA architectures: starts at 6.0 because genai code uses __hfma2 (half-precision FMA)
        # which requires Compute Capability 6.0+ (Pascal). CC 5.0 (Maxwell) only supports FP16 storage, not arithmetic.
        # Compiling with 10.0 failed with error: nvcc fatal: Unsupported gpu architecture 'compute_100' - cuda 12.6 issue ?
        - TORCH_CUDA_ARCH_LIST=6.0;7.0;7.5;8.0;8.6;9.0+PTX  # [cuda_compiler_version != "None"]
      string: cuda{{ cuda_compiler_version | replace('.', '') }}_py{{ CONDA_PY }}_h{{ PKG_HASH }}_{{ PKG_BUILDNUM }}  # [cuda_compiler_version != "None"]
    requirements:
      build:
        - {{ compiler('c') }}
        - {{ compiler('cxx') }}
        - {{ compiler('cuda') }}  # [cuda_compiler_version != "None"]
        - {{ stdlib('c') }}
        - cmake
        - make
        - ninja
        - git
        - python
        - pip
        - setuptools-git-versioning
        - pytorch
        - pytorch * *cuda*  # [cuda_compiler_version != "None"]
        - scikit-build
        - tabulate
        - jinja2
        - pyyaml
        - click
        - pyre-extensions
        - patchelf  # [linux]
        - cuda-cccl  # [cuda_compiler_version != "None"]
        - cuda-cudart-dev  # [cuda_compiler_version != "None"]
        - cuda-nvrtc-dev  # [cuda_compiler_version != "None"]
        - cuda-nvtx-dev  # [cuda_compiler_version != "None"]
        - libcublas-dev  # [cuda_compiler_version != "None"]
        - libcusolver-dev  # [cuda_compiler_version != "None"]
        - libcusparse-dev  # [cuda_compiler_version != "None"]
        - libcurand-dev  # [cuda_compiler_version != "None"]
      host:
        - python
        - pip
        - setuptools
        - setuptools-git-versioning
        - wheel
        - pytorch
        - pytorch * *cuda*  # [cuda_compiler_version != "None"]
        - scikit-build
        - numpy
        - cuda-version {{ cuda_compiler_version }}  # [cuda_compiler_version != "None"]
      run:
        - python
        - pytorch
        - numpy
        - cuda-version >={{ cuda_compiler_version }}  # [cuda_compiler_version != "None"]
        - cuda-cudart  # [cuda_compiler_version != "None"]
    test:
      imports:
        - fbgemm_gpu
        - fbgemm_gpu.experimental.gen_ai
      commands:
        - python -c "import fbgemm_gpu; print(fbgemm_gpu.__version__)"
        - python -c "import fbgemm_gpu.experimental.gen_ai"

about:
  home: https://github.com/pytorch/FBGEMM
  summary: 'FBGEMM GPU kernel libraries for PyTorch'
  description: |
    FBGEMM (Facebook GEneral Matrix Multiplication) is a low-precision, high-performance matrix-matrix multiplications and convolution library for server-side inference.
    The library provides efficient low-precision general matrix multiplication for small batch sizes and support for accuracy-loss minimizing techniques such as row-wise quantization and outlier-aware quantization. FBGEMM also exploits fusion opportunities in order to overcome the unique challenges of matrix multiplication at lower precision with bandwidth-bound operations.
  license: BSD-3-Clause
  license_family: BSD
  license_file: LICENSE.txt
  doc_url: https://github.com/pytorch/FBGEMM
  dev_url: https://github.com/pytorch/FBGEMM

extra:
  recipe-maintainers:
    - das-intensity
